# 使用pandas库来操作数据

## 1.Transforming Data

### 1.1 pandas的DataFrame数据格式有以下几种浏览方式，在进行数据分析之前可以粗略浏览：

- `.head()` returns the first few rows (the “head” of the DataFrame).

- `.info()` shows information on each of the columns, such as the data type and number of missing values.

- `.shape` returns the number of rows and columns of the DataFrame.

- `.describe()` calculates a few summary statistics for each column.

### 1.2 pandas的DataFrame数据格式有以下几种常用属性：
- `.values`: A two-dimensional NumPy array of values.
- `.columns`: An index of columns: the column names.
- `.index`: An index for the rows: either row numbers or row names.

### 1.3 pandas排序和过滤方法

- `.sort_values()`是常用的排序方法。可以对单个值进行排序，也可以根据多个值进行排序。`df.sort_values("breed")` 和 `df.sort_values(["breed", "weight_kg"])`

- `.sort_values()`有参数`ascending=`。这里可以是单一的True和False，对于存在多个排序列时，可以用列表表示他们的排列顺序。比如`ascending=[True, False]`

- `.isin()`是一个非常简单的方法。尤其是再对一列进行筛选的时候。比如我们希望选择color列，而满足我们要求的颜色有brown, black, tan。我们可以使用.isin方法。代码如下：

  ```Python
  colors = ["brown", "black", "tan"]
  condition = dogs["color"].isin(colors) # 得到的是一个包含True和False的Series
  dogs[condition]
  ```

## 2.pandas的常用统计学和聚合方法

- 常用统计学方法：

1. `.mean()`函数可以对DataFrame的列进行求平均值计算

2. `.median()`函数可以对DataFram的列进行求中位数计算
3. `.min()`函数可以对DataFrame的列进行求最小值计算
4. `.max()`函数可以对DataFrame的列进行求最大值计算
5. `.quantile()`函数可以对DataFrame的列进行求四分位数计算
6. `.cummax()`返回一个DataFrame或Series轴上的累积最大值。
7. `.cumsum()`返回一个DataFrame或Series轴上的累积和。

- 聚合方法：

1. `.agg()`函数可以把函数放入参数，使得对DataFrame的列进行聚合的函数计算

   ```Python
   # A custom IQR function
   def iqr(column):
       return column.quantile(0.75) - column.quantile(0.25)
       
   # Print IQR of the temperature_c column
   print(sales['temperature_c'].agg(iqr))
   
   print(sales['temperature_c'].agg([min, max, sum]))
   ```


2. `.drop_duplicate()`函数可以去掉重复的数据，其中`.drop_duplicate(subset=['name'])`可以去掉name列的重复值，只保留第一个留下的元素。也可以传入一个列表，只有列表中的组合重复时才会删除。比如`.drop_duplicate(subset=['store', 'type'])`

3. `.value_counts()`函数的参数设置：normalize=正则化，sort=排序

4. `.groupby()`函数可以对categories的类别进行聚合

5. `.pivot_table()`实现透视表功能，下面是使用的例子：

   - Get the mean `weekly_sales` by `type` using `.pivot_table()` and store as `mean_sales_by_type`

     ```python
     # pivot_table()的参数第一个为需要计算的数值，index就是index，index代表着依据什么类别进行分组。columns代表着列。
     mean_sales_by_type = sales.pivot_table(values='weekly_sales', index='type')
     ```

   - 使用其他聚合函数，需要添加新参数`aggfunc=`，例子如下所示：

     ```python
     # Pivot for mean and median weekly_sales for each store type
     mean_med_sales_by_type = sales.pivot_table(values='weekly_sales', index='type', aggfunc=[np.mean, np.median])
     ```

   - 使用fill_value和margins来对数据进行优化。fill_value可以指定以某一个值来填充数据中的缺失值，margins添加了行/列小计和总计

     ```python
     #  mean weekly_sales by department and type; fill missing values with 0
     sales.pivot_table(values='weekly_sales', index='department', columns='type', fill_value=0)
     
     # the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols 
     sales.pivot_table(values="weekly_sales", index="department", columns="type", fill_value=0, margins=True)
     ```


   ## 3.切片和索引

   1. 使用`set_index`方法重新设置index。

      ```python
      # 代码和参数如下
      DataFrame.set_index(keys, drop=True, append=False, inplace=False, verify_integrity=False)
      ```

      使用`set_index`也可以设置多层索引。

      ```python
      # Index temperatures by country & city
      temperatures_ind = temperatures.set_index(['country', 'city'])
      
      # List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore
      rows_to_keep = [('Brazil', 'Rio De Janeiro'), ('Pakistan', 'Lahore')] 
      
      # Subset for rows to keep
      print(temperatures_ind.loc[rows_to_keep])
      ```

   2. 使用`reset_index`可以把之前变更过的index恢复原样。

      ```python
      # 代码和参数如下
      DataFrame.reset_index(level=None, drop=False, inplace=False, col_level=0, col_fill='')
      ```

   3. 使用`sort_index`来对索引进行排序

      ```python
      # 代码和参数如下
      DataFrame.sort_index(axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, ignore_index=False, key=None)
      
      # 例子
      dogs_ind3.sort_index(level=["color", "breed"], ascending=[True, False])
      ```

   4. pandas切片注意事项：

      - You can only slice an index if the index is sorted (using `.sort_index()`).
      - To slice at the outer level, `first` and `last` can be strings.
      - To slice at inner levels, `first` and `last` should be tuples.
      - If you pass a single slice to `.loc[]`, it will slice the rows.

      ```python
      # Subset rows from India, Hyderabad to Iraq, Baghdad
      print(temperatures_srt.loc[('India', 'Hyderabad'):('Iraq', 'Baghdad')])
      
      # Subset columns from date to avg_temp_c
      print(temperatures_srt.loc[:, 'date':'avg_temp_c'])
      
      # Subset in both directions at once
      print(temperatures_srt.loc[('India', 'Hyderabad'):('Iraq', 'Baghdad'), 'date':'avg_temp_c'])
      ```

   5. 对DataFrame的index的时间进行切片操作

      如果是对已经排序好的YYYY-MM-DD索引进行选取，只选择类似'2016'等同于选择了'2016-01-01'
      
   6. 从datetime的数据类型中提取年月日

      比如dataframe的一列是'YYYY-MM-DD'。这个数值类型为datetime。使用代码`DataFrame['date'].dt.year/month/day` 就可以提取对应的年月日

## 4. 数据可视化

1. 通过导入matplotlib.pyplot库来绘制图像，具体方法如下：

   ```python
   # 首先导入必要的库
   import matplotlib.pyplot as plt
   
   # 根据nb_sold_by_size绘制柱状图，在plot函数中设置参数kind为bar
   nb_sold_by_size.plot(kind='bar') # kind还可以为line, scatter
   
   # 根据nb_sold_by_size绘制直方图，与上面不同，需要直接使用hist()函数
   nb_sold_by_size['average_price'].hist() # alpha可以设定图像的透明度, bins可以设置区间的宽度
   
   # 为直方图设置legend的方法如下
   plt.legend(['F', 'M'])
   ```

2. 判读数据中是否有NaN值存在，可以通过`isna()`函数来进行判断，使用这个函数会对数据中的每一项都进行判断。但是在处理数据量过于庞大的数据集时不太实用。可以使用`isna().any()`来进行判断，会根据每一个columns来判断当前columns中是否存在缺失值。使用`isna().sum()`同理，可以统计出每个columns中的缺失值的总数

   ```python
   # 可以通过isna()查看individual是否含有缺失值
   avocados_2016.isna()
   
   # 可以通过isna().any()查看columns是否含有缺失值
   avocados_2016.isna().any()
   
   # 可以通过isna().sum()来计算出每一个columns所含有的缺失值的数量,通过barplot来绘制出来
   avocados_2016.isna().sum().plot(kind='bar')
   ```

   

3. 去除missing value的方法：可以直接通过dropna()来去除缺失值，会把包含缺失值的整行数据清除掉，但是这对于存在大量缺失值的数据集不太理想。这个时候我们可以填充缺失值，使用`fillna()`函数来进行缺失值的填补

